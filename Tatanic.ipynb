{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic surviver classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.data_utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open('Data/train.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        train_data.append(row)\n",
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: PassengerId\n",
      "1: Survived\n",
      "2: Pclass\n",
      "3: Name\n",
      "4: Sex\n",
      "5: Age\n",
      "6: SibSp\n",
      "7: Parch\n",
      "8: Ticket\n",
      "9: Fare\n",
      "10: Cabin\n",
      "11: Embarked\n"
     ]
    }
   ],
   "source": [
    "for col,num in zip(train_data[0], range(len(train_data[0]))):\n",
    "    print('{}: {}'.format(num,col))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = train_data[1:,1]\n",
    "labels = np.float64(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    sexind = int(*np.where(data[0] == 'Sex'))\n",
    "    ageind = int(*np.where(data[0] == 'Age'))\n",
    "    trimmed_x = data[1:,(sexind,ageind)]\n",
    "    trimmed_x[trimmed_x == ''] = '0'\n",
    "    trimmed_x[trimmed_x == 'male'] = '0'\n",
    "    trimmed_x[trimmed_x == 'female'] = '1'\n",
    "\n",
    "    trimmed_x = np.float64(trimmed_x)\n",
    "\n",
    "    ages = trimmed_x[:,1]\n",
    "    ages = ages[ages != 0]\n",
    "    mean = np.mean(ages)\n",
    "    trimmed_x[:,1][trimmed_x[:,1] == 0] = mean\n",
    "\n",
    "    norm_x = np.copy(trimmed_x)\n",
    "    mean_age = np.mean(norm_x[:,1])\n",
    "    for i in range(len(norm_x[:,1])):\n",
    "        norm_x[:,1][i] = (norm_x[:,1][i] / (mean_age)) - 1\n",
    "    return norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5\n"
     ]
    }
   ],
   "source": [
    "train_x = prep_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float64,[None,1])\n",
    "x = tf.placeholder(tf.float64, [None,2])\n",
    "learn_rate = 0.0001\n",
    "batch_size = 16\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = tf.layers.dense(x, 500, activation=tf.nn.relu)\n",
    "\n",
    "l2= tf.layers.dense(l1, 50, activation=tf.nn.relu)\n",
    "\n",
    "output = tf.layers.dense(l2,1, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=output, name='entropy'), name='loss')\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "predictions = tf.cast(tf.greater(tf.sigmoid(output),.5),tf.float64)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y), tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 20, Loss= 0.6760880700556859, Accuracy= 0.796875\n",
      "epoch 0, batch 40, Loss= 0.5707455682517499, Accuracy= 0.790625\n",
      "epoch 1, batch 20, Loss= 0.5560277247486409, Accuracy= 0.821875\n",
      "epoch 1, batch 40, Loss= 0.5072581579192347, Accuracy= 0.79375\n",
      "epoch 2, batch 20, Loss= 0.5302336417233307, Accuracy= 0.825\n",
      "epoch 2, batch 40, Loss= 0.5463586674933446, Accuracy= 0.7625\n",
      "epoch 3, batch 20, Loss= 0.5312858558503499, Accuracy= 0.825\n",
      "epoch 3, batch 40, Loss= 0.5211701934284109, Accuracy= 0.775\n",
      "epoch 4, batch 20, Loss= 0.5078751320796902, Accuracy= 0.853125\n",
      "epoch 4, batch 40, Loss= 0.5132736612816504, Accuracy= 0.775\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    display_step = 20\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    num_batches = int(len(train_x)/batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        total_acc = 0\n",
    "        total_loss = 0\n",
    "        epoch_x, epoch_y = shuffle(train_x,labels)\n",
    "        for batch in range(num_batches):\n",
    "            batch_x = epoch_x[batch * batch_size:(batch*batch_size) + batch_size]\n",
    "            batch_y = epoch_y[batch*batch_size:(batch*batch_size) + batch_size]\n",
    "            batch_y =np.reshape(batch_y, (batch_y.shape[0], 1))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={x: batch_x,\n",
    "                                                                     y: batch_y})\n",
    "            total_acc += acc\n",
    "            total_loss += loss\n",
    "            if batch % display_step == 0 and batch != 0:\n",
    "                # Calculate batch loss and accuracy\n",
    "                \n",
    "                print(\"epoch {}, \".format(epoch) + \"batch \" + str(batch) + \", Loss= \" + \"{}\".format(total_loss/display_step) + \", Accuracy= \" + \"{}\".format(total_acc/display_step))\n",
    "                total_acc = 0\n",
    "                total_loss = 0\n",
    "    saver.save(sess, './80')\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    #print(\"Testing Accuracy:\", \\\n",
    "      #  sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "       #                               y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with open('Data/test.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        test_data.append(row)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "IDs = test_data[1:,0]\n",
    "test_x = prep_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./80\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"./80\")\n",
    "    pred = sess.run(predictions, feed_dict={x:test_x})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)==len(IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.csv\", 'w') as file:\n",
    "    feilds = ('PassengerId','Survived')\n",
    "    writer = csv.DictWriter(file,feilds, delimiter=',', lineterminator = '\\n' )\n",
    "    writer.writeheader()\n",
    "    for ID, prediction in zip(IDs,pred):\n",
    "        writer.writerow({'PassengerId':str(ID), 'Survived': str(int(prediction))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now with a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    sexind = int(*np.where(data[0] == 'Sex'))\n",
    "    ageind = int(*np.where(data[0] == 'Age'))\n",
    "    trimmed_x = data[1:,(sexind,ageind)]\n",
    "    trimmed_x[trimmed_x == ''] = '0'\n",
    "    trimmed_x[trimmed_x == 'male'] = '0'\n",
    "    trimmed_x[trimmed_x == 'female'] = '1'\n",
    "\n",
    "    trimmed_x = np.float64(trimmed_x)\n",
    "\n",
    "    ages = trimmed_x[:,1]\n",
    "    ages = ages[ages != 0]\n",
    "    mean = np.mean(ages)\n",
    "    trimmed_x[:,1][trimmed_x[:,1] == 0] = mean\n",
    "\n",
    "    norm_x = np.copy(trimmed_x)\n",
    "    mean_age = np.mean(norm_x[:,1])\n",
    "    for i in range(len(norm_x[:,1])):\n",
    "        norm_x[:,1][i] = (norm_x[:,1][i] / (mean_age)) - 1\n",
    "    return norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81930415263748602"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(train_x, labels)\n",
    "score = model.score(train_x, labels)\n",
    "pred = model.predict(test_x)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"results_RFC.csv\", 'w') as file:\n",
    "    feilds = ('PassengerId','Survived')\n",
    "    writer = csv.DictWriter(file,feilds, delimiter=',', lineterminator = '\\n' )\n",
    "    writer.writeheader()\n",
    "    for ID, prediction in zip(IDs,pred):\n",
    "        writer.writerow({'PassengerId':str(ID), 'Survived': str(int(prediction))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
